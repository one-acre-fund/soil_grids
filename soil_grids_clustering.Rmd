---
author: '[Matt Lowes](mailto:email@oneacrefund.org)'
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  html_notebook:
    number_sections: yes
    code_folding: hide
    theme: flatly
    toc: yes
    toc_depth: 6
    toc_float: yes
    css: static/styles.css
---
<title>Title</title>
```{r setup, include=FALSE}
#### set up
## clear environment and console
rm(list = ls())
cat("\014")

## set up some global options
# always set stringsAsFactors = F when loading data
options(stringsAsFactors=FALSE)

# show the code
knitr::opts_chunk$set(echo = TRUE)

# define all knitr tables to be html format
options(knitr.table.format = 'html')

# change code chunk default to not show warnings or messages
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

## load libraries
# dplyr and tibble are for working with tables
# reshape is for easy table transformation
# knitr is to make pretty tables at the end
# ggplot2 is for making graphs
# readxl is for reading in Excel files
# MASS is for running boxcox tests
# gridExtra is for arranging plots
# cowplot is for adding subtitles to plots
# robustbase is to run robust regressions to compensate for outliers
# car is for performing logit transformations
libs <- c("tidyverse", "knitr", "readxl", "curl", "raster", "rgdal", "ggmap", "DT")
lapply(libs, require, character.only = T, quietly = T, warn.conflicts = F)

#### define helpful functions
# define function to adjust table widths
html_table_width <- function(kable_output, width) {
  width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
  sub("<table>", paste0("<table>\n", width_html), kable_output)
}
options(readr.show_progress = FALSE)
select <- dplyr::select
```

# Objectives

We're trying to make soil classes for 1AF Kenya to inform the placement of on-farm nutrient omission trials. This analysis accesses data from the [AfSIS soil grids project](http://data.isric.org/geonetwork/srv/eng/catalog.search#/home), combines them into a feature set and identifies clusters across sampling units to inform trial placement. 

This analysis accesses data from the [Africa SoilGrids](http://data.isric.org/geonetwork/srv/eng/catalog.search#/home) produced and hosted by ISRIC for the AfSIS project.

# Data

Most of the data have been zipped. I'm just goint to manually unzip it to simplify the process. Here's the complete list of the data I've tried to access from the online repository:

* Africa SoilGrids pH (predictions for pH H2O (10) for individual depth intervals (0-5, 5-15, 15-30 cm); make weighted average 
    + **af_PHIHOX_T__M_sd1_250m.tif**
* Africa SoilGrids organic C (predictions for SOC (g/kg) for individual depth intervals (0-5, 5-15, 15-30 cm); make weighted average 
    + **af_ORCDRC_T__M_sd1_250m.tif**
* Africa SoilGrids total N 2017 (predictions for TN (mg/kg) for 0-30 cm   
    + **af250m_nutrient_n_m_agg30cm.tif**
* Africa SoilGrids texture (textural classes derived from predictions for sand, silt & clay for individual depth intervals (0-5, 5-15, 15-30 cm); make weighted average 
    + **af_TEXMHT_T__M_sd1_250m.tif**
* Africa SoilGrids root zone texture (textural classes derived from predictions for sand, silt & clay for 0-30 cm (also available for the rootable depth) with a resolution of 1km 
    + **af_agg_30cm_TEXCLSS__M_1km.tif**
* Africa SoilGrids sand (predictions for sand content (w%) for individual depth intervals (0-5, 5-15, 15-30 cm); make weighted average 
    + **af_SNDPPT_T__M_sd1_250m.tif**
* Africa SoilGrids silt (predictions for silt content (w%) for individual depth intervals (0-5, 5-15, 15-30 cm); make weighted average 
    + **af_SLTPPT_T__M_sd1_250m.tif**
* Africa SoilGrids clay (predictions for clay content (w%) for individual depth intervals (0-5, 5-15, 15-30 cm); make weighted average 
    + **af_CLYPPT_T__M_sd1_250m.tif**
* Africa SoilGrids CEC (predictions for CEC (cmolc/kg) for individual depth intervals (0-5, 5-15, 15-30 cm); make weighted average 
    + **af_CEC_T__M_sd1_250m.tif**
* Africa SoilGrids P (predictions for available P, according to Mehlich 3 (mg/100kg; thus divide by 100 for ppm) for 0-30 cm 
* Africa SoilGrids P (predictions for total P (mg/kg) for 0-30 cm         
    + **af250m_nutrient_p_t_m_agg30cm.tif**
* Africa SoilGrids root zone coarse (predictions for coarse fragments content (v%) for 0-30 cm (also for rootable depth) at 1km resolution 
    + **af_agg_30cm_CRFVOL__M_1km.tif**
* Africa SoilGrids root zone (derived maps for rootable depth (cm), and other properties aggregated over 0-30 cm as well as over root zone depth, at 1 km resolution 
    + **af_ERZD__M_1km.tif**

And Johan clarified that the weights refer to amount of soil depth:
the weights refer to the depth intervals :

* 0-5 = 5/30 - this refers to `sd1` in the file names
* 5-15 = 10/30 - this refers to `sd2` in the file names
* 15-30 = 15/30 - this refers to `sd3` in the file names

```{r}
forceUpdate <- FALSE

firstWeight <- 5/30
secondWeight <- 10/30
thirdWeight <- 15/30

weightList <- list(firstWeight, secondWeight, thirdWeight)
```


```{r}
rawDir <- normalizePath(file.path("..", "soil_grids_raw_data"))

dataDir <- normalizePath(file.path("..", "soil_grids_data"))

tifFiles <- paste0(rawDir, "/", list.files(rawDir, pattern = ".tif$"))

```

I want to import all of these rasters but it's important that they be clearly grouped so that I can create weighted layers for all N, C, P, etc. before combining them into a single data frame. The following functions will import the selected rasters then perform the necessary calculations to prepare that soil nutrient feature for the end analysis.

The overall soil rasters are too big to do the calculations first (even if I'm doing them as efficiently as possible which I may not be). Therefore I first need to trim the larger raster down to just Kenya before calculating the weights and averages.

## Site points

I'll use the 2019 site GPS to identify and then extract the site value of each cluster to facilitate assignment of trial officers. This means that after clustering I need to convert the cluster data back into raster so that I can extract the values for the GPS points. Additionally, I need to convert the provided site [GPS points to the correct projection](http://rspatial.org/spatial/rst/6-crs.html) (from no projection? Figure out how to do this.)

It's a bit of a hack but I'm assuming a certain CRS because I don't know in what system our lat lon data are collected. 

```{r}
siteGpsFiles <- list.files(paste(dataDir, "2019", sep = "/"), pattern = ".xls")

siteGpsFiles <- siteGpsFiles[!siteGpsFiles %in% c("District Offices.xlsx", "Regional Office.xlsx", "Warehouses.xlsx")]
siteGpsFiles <- paste(dataDir, "2019", siteGpsFiles, sep = "/")

readGpsFiles <- lapply(siteGpsFiles, function(x){
  read_xlsx(x)
})

combineGpsFiles <- as.data.frame(do.call(rbind, readGpsFiles))

# investigate the projection of these data points
siteSp <- SpatialPointsDataFrame(coords = combineGpsFiles[,c("Longitude", "Latitude")], data = combineGpsFiles, proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")) # use this to get the right map for N and P

siteReproject <- spTransform(siteSp, CRS("+proj=laea +lat_0=5 +lon_0=20 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"))
```

## Kenya boundaries

Determine the Kenya boundaries we need by using `intersect`.

```{r}
if(!file.exists(paste0(dataDir, "/GADM_2.8_KEN_adm2.rds"))){
  keBoundaries <- getData("GADM", country='KE', level=2, path = dataDir) 
} else {
  keBoundaries <- readRDS(paste0(dataDir, "/GADM_2.8_KEN_adm2.rds"))
}

# I additionally want to subset this down to just western Kenya because it's too computationally intentsive to run these calculations for other areas. I'm going to subset the shape file to just western Kenya and the plot it to confirm I've done it correctly.
mapNandP <- intersect(keBoundaries, siteSp)

keTransform <- spTransform(keBoundaries, CRS("+proj=laea +lat_0=5 +lon_0=20 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"))

westernMap <- intersect(keTransform, siteReproject) # need to intersect keBoundariesOriginal map for N and P
```

## Applying weights

It's not going to be very elegant but I'm going to manually apply the weights to the soil layers because I can't figure out a more efficient way to do this. Ultimately I probably want that in a separate data preparation script but for the time being I'll just have it save a final file that I won't refresh unless I need to. Here's a [helpful reference](https://gis.stackexchange.com/questions/61243/clipping-a-raster-in-r).

```{r}
tifFinder <- function(input){
  # finds the right layers from the full set of input data, tifFiles
  #outputs list of those files.
  tifFiles[grep(input, tifFiles)]
}


importRasterAsList <- function(listInput){
  # import the relevant files for the soil layer
  # outputs a raster list
  rasterFile = lapply(listInput, function(x) raster(x))
}

addWeights <- function(rasterList, weightList){
  
  # confirm that this approach gives the same result as doing it manually.
  # confirmed that this is leading to same result!
  
  # make sure it's in order!
    r = Map("*", rasterList, weightList)
    return(r)
  
}
```

## Cropping data

`Crop` extent of the raster and then `mask` raster to fit the outlines of Kenya. Hopefully this doesn't take too long? Not too long! This step is necessary or else the raster is too big for the later steps. 

```{r}
getKenyaValues <- function(rasterList, map){
  # this function trims the africa sized raster down to just the extent in western Kenya we care about
  # output: a list of rasters to go into the weight calculations
  # this cna also be used on a single raster but it'll then return 
  rlCropped <- lapply(rasterList, function(x){crop(x, extent(map))})
  rlMask <- lapply(rlCropped, function(x){mask(x, map)})
  
  return(rlMask)
}

#soilExtract <- extract(soilMask, westernMap, df = TRUE) # gets soil values for each of the 87 shapes in the westernMap
```

```{r}
# I can't hold all the raw data in memory so I'll run these one at a time and then only keep the analyzed results on the hd.

# ph but with functions
if(!file.exists(paste(dataDir, "output", "pH_weighted_layer.tif", sep = "/")) | forceUpdate){
  print("tried")
  
  pHLayer <- sum(stack(addWeights(getKenyaValues(importRasterAsList(tifFinder("af_PHIHOX_T__M")), westernMap), weightList)))
  
  writeRaster(pHLayer, file = paste(dataDir, "output", "pH_weighted_layer.tif", sep = "/"), format="GTiff", overwrite=TRUE)
} else {
  pHLayer <- raster(paste(dataDir, "output", "pH_weighted_layer.tif", sep = "/"))
}

# carbon
if(!file.exists(paste(dataDir, "output", "carbon_weighted_layer.tif", sep = "/")) | forceUpdate){
  carbonLayer <- sum(stack(addWeights(getKenyaValues(importRasterAsList(tifFinder("af_ORCDRC_T__M")), westernMap), weightList)))
  
  writeRaster(carbonLayer, file = paste(dataDir, "output", "carbon_weighted_layer.tif", sep = "/"), format="GTiff", overwrite=TRUE)
} else {
  carbonLayer <- raster(paste(dataDir, "output", "carbon_weighted_layer.tif", sep = "/"))
}

# texture
if(!file.exists(paste(dataDir, "output", "texture_weighted_layer.tif", sep = "/")) | forceUpdate){
  textureLayer <- sum(stack(addWeights(getKenyaValues(importRasterAsList(tifFinder("af_TEXMHT_T__M")), westernMap), weightList)))
  
  writeRaster(textureLayer, file = paste(dataDir, "output", "texture_weighted_layer.tif", sep = "/"), format="GTiff", overwrite=TRUE)
} else {
  textureLayer <- raster(paste(dataDir, "output", "texture_weighted_layer.tif", sep = "/"))
}

# sand content
if(!file.exists(paste(dataDir, "output", "sand_weighted_layer.tif", sep = "/")) | forceUpdate){
  sandLayer <- sum(stack(addWeights(getKenyaValues(importRasterAsList(tifFinder("af_SNDPPT_T__M")), westernMap), weightList)))
  
  writeRaster(sandLayer, file = paste(dataDir, "output", "sand_weighted_layer.tif", sep = "/"), format="GTiff", overwrite=TRUE)
} else {
  sandLayer <- raster(paste(dataDir, "output", "sand_weighted_layer.tif", sep = "/"))
}
# silt content
if(!file.exists(paste(dataDir, "output", "silt_weighted_layer.tif", sep = "/")) | forceUpdate){
  siltLayer <- sum(stack(addWeights(getKenyaValues(importRasterAsList(tifFinder("af_SLTPPT_T__M")), westernMap), weightList)))
  
  writeRaster(siltLayer, file = paste(dataDir, "output", "silt_weighted_layer.tif", sep = "/"), format="GTiff", overwrite=TRUE)
} else {
  siltLayer <- raster(paste(dataDir, "output", "silt_weighted_layer.tif", sep = "/"))
}

# clay content
if(!file.exists(paste(dataDir, "output", "clay_weighted_layer.tif", sep = "/")) | forceUpdate){
  clayLayer <- sum(stack(addWeights(getKenyaValues(importRasterAsList(tifFinder("af_CLYPPT_T__M")), westernMap), weightList)))
  
  writeRaster(clayLayer, file = paste(dataDir, "output", "clay_weighted_layer.tif", sep = "/"), format="GTiff", overwrite=TRUE)
} else {
  clayLayer <- raster(paste(dataDir, "output", "clay_weighted_layer.tif", sep = "/"))
}

# CEC
if(!file.exists(paste(dataDir, "output", "cec_weighted_layer.tif", sep = "/")) | forceUpdate){
  cecLayer <- sum(stack(addWeights(getKenyaValues(importRasterAsList(tifFinder("af_CEC_T__M")), westernMap), weightList)))
  
  writeRaster(cecLayer, file = paste(dataDir, "output", "cec_weighted_layer.tif", sep = "/"), format="GTiff", overwrite=TRUE)
} else {
  cecLayer <- raster(paste(dataDir, "output", "cec_weighted_layer.tif", sep = "/"))
}
  
```

## Importing other layers

This summary repeats content from above:

* Africa SoilGrids root zone texture (textural classes derived from predictions for sand, silt & clay for 0-30 cm (also available for the rootable depth) with a resolution of 1km 
    + **af_agg_30cm_TEXCLSS__M_1km.tif**
* Africa SoilGrids P (predictions for available P, according to Mehlich 3 (mg/100kg; thus divide by 100 for ppm) for 0-30 cm 
* Africa SoilGrids P (predictions for total P (mg/kg) for 0-30 cm         
    + **af250m_nutrient_p_t_m_agg30cm.tif**
* Africa SoilGrids root zone coarse (predictions for coarse fragments content (v%) for 0-30 cm (also for rootable depth) at 1km resolution 
    + **af_agg_30cm_CRFVOL__M_1km.tif**
* Africa SoilGrids root zone (derived maps for rootable depth (cm), and other properties aggregated over 0-30 cm as well as over root zone depth, at 1 km resolution 
    + **af_ERZD__M_1km.tif**
* Africa SoilGrids total N 2017 (predictions for TN (mg/kg) for 0-30 cm   
    + **af250m_nutrient_n_m_agg30cm.tif**
    
    
I'm importing these directly because it's not computationally intensive to do so.

## N and P

The nitrogen and phosphorous layers are coming in with different projections. I'm going to import them here and see if I can truncate them as need be using the original, untransformed shape file.

```{r}
if(!file.exists(paste(dataDir, "output", "phosphorous_layer_projected.tif", sep = "/")) | forceUpdate){
  
  phosLayer <- getKenyaValues(importRasterAsList(tifFinder("af250m_nutrient_p_t_m_agg30cm.tif")), mapNandP)[[1]] 
  phosLayer <- projectRaster(phosLayer, pHLayer)
  
  writeRaster(phosLayer, file = paste(dataDir, "output", "phosphorous_layer.tif", sep = "/"), format="GTiff", overwrite=TRUE)
} else {
  phosLayer <- raster(paste(dataDir, "output", "phosphorous_layer_projected.tif", sep = "/"))
}


if(!file.exists(paste(dataDir, "output", "nitrogen_layer_projected.tif", sep = "/")) | forceUpdate){
  
  nitrogenLayer <- getKenyaValues(importRasterAsList(tifFinder("af250m_nutrient_n_m_agg30cm.tif")), mapNandP)[[1]] 
  # and then reproject it to the right system
  nitrogenLayer <- projectRaster(nitrogenLayer, pHLayer)
  
  writeRaster(nitrogenLayer, file = paste(dataDir, "output", "nitrogen_layer_projected.tif", sep = "/"), format="GTiff", overwrite=TRUE)
} else {
  nitrogenLayer <- raster(paste(dataDir, "output", "nitrogen_layer_projected.tif", sep = "/"))
}
```

Time is short so I want to just set the extent of these additional layers to that of the weighted layers. I may be doing something wrong but I don't think I am.

```{r}
zoneTextureLayer <- getKenyaValues(importRasterAsList(tifFinder("af_agg_30cm_TEXCLSS__M_1km.tif")), westernMap)[[1]]
zoneTextureLayer <- projectRaster(zoneTextureLayer, pHLayer)

zoneCoarseLayer <- getKenyaValues(importRasterAsList(tifFinder("af_agg_30cm_CRFVOL__M_1km.tif")), westernMap)[[1]]
zoneCoarseLayer <- projectRaster(zoneCoarseLayer, pHLayer)

zoneLayer <- getKenyaValues(importRasterAsList(tifFinder("af_ERZD__M_1km.tif")), westernMap)[[1]]
zoneLayer <- projectRaster(zoneLayer, pHLayer)
```

## Stacking data

And then check that the extent of the phosphorous layer is the same as the ohter layers so they can all fit in one stack together. **I'm currently excluding the phosphorous layer as it was taking forever to reproject in the right projection**. Moving on!

An option for a future update is to utilize a package like clusteR to run the re-projection arcoss multiple clusters. But not today!

```{r, fig.height=6, fig.width=6}
checkExtent <- extent(pHLayer)

lapply(list(carbonLayer, sandLayer, clayLayer, siltLayer, cecLayer, nitrogenLayer, phosLayer, zoneTextureLayer, zoneCoarseLayer, zoneLayer), function(soil_layer){
  return(extent(soil_layer) == checkExtent)
})

# remove phosLayer from the soil stack but I'll eventually want to come back and figure this out

soilStack <- stack(pHLayer, carbonLayer, sandLayer, clayLayer, siltLayer, cecLayer, nitrogenLayer, zoneTextureLayer, zoneCoarseLayer, zoneLayer)
names(soilStack) <- c("pH", "carbon", "sand", "clay", "silt", "cec", "nitrogen", "zoneTexture", "zoneCoarse", "zone")
plot(soilStack)
```

# Clustering Code

The main steps here will be:

* Convert the raster data to a data frame so that I can create the clusters
* Use the existing k-means code to create clusters
* Identify the tipping point in the graph

This [website details the process of converting raster to data frame](https://www.rdocumentation.org/packages/raster/versions/2.8-4/topics/as.data.frame)

And then per Step's requests:

* How we should stratify our sampling across those clusters with our 9 trial officers
* A list of site options for each trial officer. Site options need to be:
    + away from cluster boundaries (e.g. create a shape file of each cluster, then trim 10% off margins)
    + spread out as much as possible within the cluster (if we are assigning >1 trial officer to any cluster)
    + ideally close to 2 other sites…if this is too much, if we have districts which are all well within any cluster, I think that might be enough for Kenya PI to work with for now.

This converts the grids into points but in some way I don't really care what each point is called. But I need a way to connet each point back to something interpretable.

Follow this example for [converting to df, executing clustering, and saving the results back as a raster](https://gis.stackexchange.com/questions/123639/unsupervised-classification-with-kmeans-in-r). I'll need the data back in raster form to extract the values for the sites. I'll want to assign the cluster values as new layers in the raster stack, I assume and then be able to extract those for the given GPS.

Actually what I want to do is convert the cluster assignment list to rasters and then add them as layers to the `soilStack`! Hopefully that works.

# Clustering analysis

I followed this methodology for outputing k means results as raster data to simplify the placement process: [implement k-means with raster data](https://www.r-exercises.com/2018/02/28/advanced-techniques-with-raster-data-part-1-unsupervised-classification/) 

```{r}
numClusters <- 2:10

runKMeansRaster <- function(dat, numClusters=2:8) {
  # this function executes k-means clustering. I'm modifying it from the original code in the global_aez and country_aez R files to work with the raster data. This just works with the soil data in raster format. It might be better to functionize the input selection more but I'm just going to hardcode it for now.
  # the NA process will hopefully not complicate this too much considering that the NAs might go back in different places in each file.
  # input >> raster stack, 
  # output - list of two items, k means cluster assignments in raster form and the kmeans output so I can analyze which scenario is the best
  datA = values(dat)
  #datA = datA[,c(datCols)]
  
  idx = complete.cases(datA)
  
  datB = datA[idx,]
  scaled <- scale(datB)
  
  kClusts <- lapply(numClusters, FUN=function(k) {
    
    set.seed(2017201616)
    clusterRes = kmeans(scaled, centers=k, nstart=50, iter.max=150, algorithm = "MacQueen")
    
    # initiate new holding vector for output that's the same size as the original raster stack
    kmClustRes = vector(mode = 'integer', length = ncell(dat))
    kmClustRes[!idx] <- NA
    kmClustRes[idx] <- as.factor(clusterRes$cluster)
    
    clusterRaster <- raster(dat[[1]])
    values(clusterRaster) <- kmClustRes
    return(list(clusterRaster, clusterRes))
  })
  return(kClusts)
}

```

Address [runtime error in kmeans with `gc()`](https://stackoverflow.com/questions/21382681/kmeans-quick-transfer-stage-steps-exceeded-maximum) or possibly other solutions if need be.

```{r}
#soilCols <- names(soilStack)[!names(soilStack) %in% c("x", "y")]
gc()
if(!file.exists(paste(dataDir, "soilClusters.rds", sep = "/")) | forceUpdate){
  soilClusters <- runKMeansRaster(soilStack, numClusters)
  saveRDS(soilClusters, paste(dataDir, "soilClusters.rds", sep = "/"))
} else {
  soilClusters <- readRDS(paste(dataDir, "soilClusters.rds", sep = "/"))
}

clusterRasters <- lapply(soilClusters, function(x){x[[1]]})

soilKmeans <- lapply(soilClusters, function(x){x[[2]]})

plot(numClusters, lapply(soilKmeans, FUN=function(x) { sum(x$withinss) }),   
     type="b", ylab="average within cluster sum of square error",
     main="Clustering soil at 250m grid level")
```

> **Interpretation**: The most evident kink in the graph is at 3 clusters. This changed from the previous version where we were not able to include N and P.

# Map

My hope is that I can just simply use ggplot to make a simple graph of the clusters to avoid having to actually plot them on a map. Let's see how this goes.

```{r}
for(i in 1:length(clusterRasters)){
  #values(clusterRasters[[i]]) <- as.factor(values(clusterRasters[[i]]))
  print(plot(clusterRasters[[i]]))
}

```

> As you can see we're not getting clean delineation between zones on the map. This shouldn't be entirely surprising but it potentially complicates the process of placing trial officers.

**Question:** Are we okay with the AEZ being non-contiguous? 

# Site extraction

```{r}
fullStack <- stack(soilStack, stack(clusterRasters))

siteClusterValues <- lapply(clusterRasters, function(x){
  clusterVal <- extract(x,
          siteReproject,
          buffer = 1, # adjust the buffer to ensure I capture all the points. I'm not sure what the unit is here.
          fun = max,
          df = TRUE)
  
  return(data.frame(siteReproject, clusterVal))
})

# this takes too long to run
# siteClusterValues <- extract(fullStack,
#           siteReproject,
#           buffer = 10,
#           fun = max,
#           df = TRUE)

scenarioDat <- do.call(rbind, lapply(siteClusterValues, function(clusterDat){
  clusterDat$totalCluster  = max(clusterDat$layer, na.rm = T)
  return(clusterDat)
})) %>%
  rename(cluster = layer)
```


# Summary

Summarize the clusters in each scenario. There are some sites that are not found in the raster data which means that I may need to change the shape file.

```{r}
scenarioDat %>%
  filter(totalCluster == 2) %>%
  filter(!is.na(cluster)) %>%
  ggplot(., aes(x = Longitude.1, y = Latitude.1, color = as.factor(cluster))) + 
  geom_point()
```

It seems that most of the missing points are in regions that are not currently included. Try to include them. There are oddly though some points that 

I need to be able to extract the soil values for the site points in order to actually summarize the cluster values. That code was taking too long to run so I'm just summarizing the distribution of the clusters.

```{r eval=FALSE, include=FALSE}
scenarioDat %>%
  filter(!is.na(cluster)) %>%
  group_by(totalCluster, cluster) %>%
  summarise_at(vars(matches("layer")),
               funs(mean))
```

## Ratio of points per cluster

First, let's look at all:

```{r}
ratioTab <- scenarioDat %>%
  group_by(totalCluster, cluster) %>%
  summarize(n = n()) %>%
  mutate(freq = n / sum(n),
         freqFormat = paste0(round(freq,2) * 100, "%"),
         officerCount = round(freq * 9,1))

kable(ratioTab, format = 'markdown')
```

Then the 3 AEZ scenario

```{r}
ratioTab %>%
  filter(totalCluster == 3) %>%
  kable(., format = 'markdown')
```

## Trial placement

The table above shows the rough breakdown of trial officers across the AEZs. The breakdowns are obviously imprecise but can suggest where we should place trial officers.

It'll be a bit difficult to empirically identify sites that are 'more' of one AEZ than others. The table below lets you explore which sites appear in which clusters. I'm also going to output the full excel file to allow you to filter the data yourself.

```{r}
scenarioDat %>%
  filter(totalCluster == 3) %>%
  select(-c(Latitude, Latitude.1, Longitude, Longitude.1, Accuracy, optional, ID)) %>%
datatable(., filter = 'top', options = list(
  searching = TRUE,
  pageLength = 20
))
```

```{r}
scenarioDat %>% write.csv(., file=paste(dataDir, "siteAssignment.csv", sep = "/"))

```




